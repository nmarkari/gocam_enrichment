{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d397e2bf-139b-4efa-95aa-0105ed34f8e5",
   "metadata": {},
   "source": [
    "Modified ncHGT.py file that doesnt call BiasedUrn and instead returns the number of calls that would be made to BiasedUrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b747896d-858c-4c05-9efd-5a876519246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../dev')\n",
    "\n",
    "import rpy2\n",
    "from rpy2.robjects.packages import importr\n",
    "BiasedUrn = importr('BiasedUrn')\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "def get_M_wM():\n",
    "    \"\"\" returns M, the number of entities in the background, and w_M, the mean size of entities in the background\"\"\"\n",
    "    setID2members = utils.csv2dict('../data/setID2members.csv')\n",
    "    l = []\n",
    "    for s,m in setID2members.items():\n",
    "        l.append(len(m))\n",
    "    l = np.array(l)\n",
    "    l = np.sort(l)\n",
    "    num_empty_sets = np.sum(l==0)\n",
    "    \n",
    "    l = l[l!=0]\n",
    "    mean = np.mean(l)#l[4:-4]) 1% trimmed mean?\n",
    "    num_sets = len(l)\n",
    "    bg = len(utils.csv2dict('../data/ID2gocam_mouse.csv'))\n",
    "    M = bg-num_empty_sets\n",
    "    \n",
    "    w_M = np.round(((M-num_sets)+num_sets*mean)/M,decimals=2)\n",
    "    return M, w_M\n",
    "\n",
    "def make_initial_vectors(gocam2ID,setID2members, gc, M,w_M):\n",
    "    \"\"\"initializes counts vector (m) and weights vector (w), where each entity gets its own element in the arrays\n",
    "- values in m only take on 0 (if there is no solo proteins) or 1\n",
    "- values in w correspond to the weight of each element in m (weighted by the # genes in a set or 1 for solo proteins)\"\"\"\n",
    "    w_gc = [1] #initialize with 1 as the weight of single proteins (irrespective of whether there are any)\n",
    "    m_gc = [0] #initialize with 0 single proteins\n",
    "    num_protein = 0\n",
    "    for i in gocam2ID.get(gc):\n",
    "        if \"sset:\" in i:\n",
    "            w_i = len(setID2members.get(i))\n",
    "            w_gc.append(w_i)\n",
    "            m_gc.append(1)\n",
    "        else:\n",
    "            num_protein+=1\n",
    "    m_gc[0] = num_protein\n",
    "    m_gc.append(M-np.sum(m_gc)) #entities not in the gocam (roughly)\n",
    "    w_gc.append(w_M) #weight for entities not in the gocam (all weighted as w_M (the mean))\n",
    "    return w_gc, m_gc\n",
    "\n",
    "\n",
    "def make_new_vectors(w_gc,m_gc,M,w_M):\n",
    "    \"\"\"compress the m and w vectors by grouping elements according to their weights\n",
    "- w is the ordered set of unique weights for entities of the gocam + the background bin\n",
    "- m[i] is the number of entities in the pathway with the weight specified in w[i] + the background bin\"\"\"\n",
    "    w_temp = w_gc[:-1]\n",
    "    if w_temp[0] != 1:\n",
    "        print('Possible bug: w_temp[0] != 1',w_temp)\n",
    "        \n",
    "    w_new, m_temp = np.unique(w_temp, return_counts=True)\n",
    "    m_temp[0]=m_gc[0] #w_gc and m_gc have weight 1 as w_gc[0] and the number of single proteins as m_gc[0]\n",
    "    m_new = np.append(m_temp,np.array([M-np.sum(m_temp)]))\n",
    "    w_new = np.append(np.unique(w_temp),np.array([w_M]))\n",
    "    return w_new, m_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ncHGT_sf(XT,m,N,w):\n",
    "    \"\"\"survival function, sums PMF for all possibilities where K >= k by calling BiasedUrn\"\"\"\n",
    "    #l = len(XT)/len(m)\n",
    "    if len(XT) == 0:\n",
    "        print('len(XT) = 0')\n",
    "        return -1\n",
    "    pval = 0\n",
    "    np.seterr(under='warn')\n",
    "    for i in range(len(XT)):\n",
    "        x = rpy2.robjects.IntVector(XT[i])\n",
    "        pval = pval + BiasedUrn.dMFNCHypergeo(x,m,N,w, precision = 1e-10)[0]\n",
    "    return pval\n",
    "\n",
    "\n",
    "def enumerate_possibilities(m_new,i,prev_array):\n",
    "    \"\"\"enumerate all possible counts vectors\"\"\"\n",
    "    first = True\n",
    "    for j in range(m_new[i]+1):\n",
    "        xt = prev_array.copy()\n",
    "        xt[0][i] = j\n",
    "        \n",
    "        #recursion\n",
    "        if (i < len(m_new)-1):\n",
    "            xt = enumerate_possibilities(m_new, i+1, xt) #will return matrix (array of arrays)\n",
    "            \n",
    "        #combining results into matrix\n",
    "        if not first:\n",
    "            XT = np.concatenate([XT,xt], axis = 0)\n",
    "        else:\n",
    "            XT = xt\n",
    "            first = False\n",
    "    return XT\n",
    "\n",
    "\n",
    "def do_ncHGT(k,gc,M,N, compress = True):\n",
    "    setID2members = utils.csv2dict('../data/setID2members.csv')\n",
    "    gocam2ID = utils.csv2dict('../data/gocam2ID_mouse.csv')\n",
    "    \n",
    "    M, w_M = get_M_wM()\n",
    "    \n",
    "    #make weight (w) and bin size (m) vectors where each entity in the gocam gets its own entry\n",
    "    w_in, m_in = make_initial_vectors(gocam2ID, setID2members, gc, M,w_M)\n",
    "    \n",
    "    #update m and w vectors by grouping sets of the same size\n",
    "    w_new = w_in\n",
    "    m_new = m_in\n",
    "    if compress:\n",
    "        w_new , m_new= make_new_vectors(w_in,m_in,M,w_M)\n",
    "\n",
    "    #make XT matrix, an enumeration of all possible arangements of balls in bins based on m_new and w_new\n",
    "    m_gc = m_new[:-1] #don't pass the background bin to XT\n",
    "    XT = enumerate_possibilities(m_gc,0,np.zeros(shape=(1,len(m_gc))))\n",
    "    \n",
    "    #filter XT to only include the region of the sample space >= k (which is what we want to sum probabilities over)\n",
    "    mask1 = (np.sum(XT, axis=1) >= k)\n",
    "    XT = XT[mask1]\n",
    "\n",
    "    #filter XT to ensure that more than N entities are not picked\n",
    "    mask2 = (np.sum(XT, axis=1) <= N)\n",
    "    XT = XT[mask2]\n",
    "\n",
    "    #add the remaining entities to the m+1th bin (non gocam bin)\n",
    "    x_mp1_vec = N- np.sum(XT, axis = 1) #number of balls to be drawn from the last bin (the non-gocam background)\n",
    "    XT = np.concatenate((XT,x_mp1_vec.reshape(len(x_mp1_vec),1)), axis = 1)\n",
    "\n",
    "    m = rpy2.robjects.IntVector(m_new)\n",
    "    w = rpy2.robjects.FloatVector(w_new)\n",
    "    size = len(XT)\n",
    "    return size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21e786-4ca6-449f-8868-762c94874e05",
   "metadata": {},
   "source": [
    " Modified enrich.py file to make calls to the above ncHGT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c775de-a3e5-41be-a14d-f9c262ee29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import hypergeom\n",
    "import sys\n",
    "sys.path.append('../GOCAM_Project/dev')\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "def get_sizes (data): #data= dataframe with gocam IDs and gene identifiers as columns\n",
    "    return data['gocam'].value_counts()\n",
    "    \n",
    "def get_sets (gene_list):\n",
    "    sets = []\n",
    "    not_in_a_set = []\n",
    "    members2setID = utils.csv2dict('../data/members2setID.csv')\n",
    "    setID2members_input = {}\n",
    "    for g in gene_list:\n",
    "        s = members2setID.get(g)\n",
    "        if s != None:\n",
    "            sets = sets +s\n",
    "            for i in s:\n",
    "                if (i in setID2members_input) == False:\n",
    "                    setID2members_input[i]={g}\n",
    "                else:\n",
    "                    prev = setID2members_input.get(i)\n",
    "                    prev.add(g)\n",
    "                    setID2members_input[i] = prev\n",
    "        else:\n",
    "            not_in_a_set.append(g)\n",
    "    return not_in_a_set, list(set(sets)),setID2members_input #remove duplicates\n",
    "\n",
    "def filter_gene_list(gene_list, Dict):\n",
    "    filtered_gene_list = []\n",
    "    filtered_out = []\n",
    "    for gene in gene_list:\n",
    "        if gene in Dict:\n",
    "            filtered_gene_list.append(gene)\n",
    "        else:\n",
    "            filtered_out.append(gene)\n",
    "    return filtered_out, filtered_gene_list\n",
    "\n",
    "def count_genes(gene_list, Dict):\n",
    "    gocam_counts = {} #key=gocam, value=list of genes in gocam that are also in the user's list\n",
    "    for g in gene_list: \n",
    "            gocams = Dict.get(g)\n",
    "            for gocam in gocams:\n",
    "                if (gocam in gocam_counts) == False:\n",
    "                    gocam_counts[gocam]=[g]\n",
    "                else:\n",
    "                    prev = gocam_counts.get(gocam)\n",
    "                    prev.append(g)\n",
    "                    gocam_counts[gocam] = prev\n",
    "    return gocam_counts\n",
    "\n",
    "#BENJAMINI HOCHBERG CORRECTION applied in correct_pval_and_format()\n",
    "#ncHGT is either False (indicating that regular HGT should be done) or a positive integer denoting N for ncHGT\n",
    "def hgt(counts, gocam_sizes, FDR, gene_list_size, background_gene_list_size, ncHGT = False, compress = True):\n",
    "    \"\"\" ncHGT is either False (for set or standard methods) or corresponds to N \"\"\"\n",
    "    num_calls = []\n",
    "    iterator = tqdm.tqdm(counts.items())\n",
    "    for gocam, gene_list in iterator:\n",
    "        count = len(gene_list) \n",
    "        gocam_size = gocam_sizes[gocam]\n",
    "        pvalue = None\n",
    "        if ncHGT:\n",
    "            if count <=1: #avoid unnecessary calls to BiasedUrn due to computation time\n",
    "                pvalue = 1\n",
    "            else:\n",
    "                r = do_ncHGT(count -1,gocam,background_gene_list_size,ncHGT, compress = compress)\n",
    "                num_calls.append(r)\n",
    "    return num_calls\n",
    "\n",
    "#Benjamini Hochberg correction\n",
    "def correct_pval_and_format(enriched_gocams, background_num_gocams,show_significant,FDR):\n",
    "    df = pd.DataFrame(enriched_gocams, columns =['url', 'pval (uncorrected)', '# entities in list','#entities in model','shared entities in gocam'])\n",
    "    df.sort_values('pval (uncorrected)',inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['FDR_val'] = (df.index+1)*FDR/background_num_gocams\n",
    "    df['Less_than'] = (df['pval (uncorrected)'] < df['FDR_val'])\n",
    "    index = df.Less_than.where(df.Less_than==True).last_valid_index()\n",
    "    df_significant = df\n",
    "    if (show_significant):\n",
    "        df_significant = df.loc[0:index].copy()\n",
    "        if index == None:\n",
    "            df_significant = pd.DataFrame(columns =['url', 'pval (uncorrected)', '# entities in list','#entities in model','shared entities in gocam'])\n",
    "    df_display = df_significant[['url','pval (uncorrected)', '# entities in list', '#entities in model','shared entities in gocam']].copy()\n",
    "    #modelID2title = pd.read_csv('../data/modelID2title_mouse.csv')\n",
    "    temp = pd.read_csv('../data/modelID2title_mouse.csv',header = 0,names=['gocam','title'])\n",
    "    modelID2title = pd.Series(temp.title.values,index=temp.gocam).to_dict()\n",
    "    df_display['title'] = df_display['url'].map(modelID2title)\n",
    "    cols = df_display.columns.to_list()\n",
    "    cols[0]='title'\n",
    "    cols[-1]='url'\n",
    "    df_display = df_display[cols]\n",
    "    return df_display\n",
    "\n",
    "#Dict can only contain 1 instance of each gene per gocam (no duplicates)\n",
    "#show_significant only affects the multiple testing correction. If the uncorrected pval > FDR, hgt() will already remove it\n",
    "def enrich(gene_list, uni_list,uniprot2input,gocam_sizes, Dict, ncHGT=False, show_significant=True,FDR=.05, compress = True):\n",
    "    background_gene_list_size = len(Dict)\n",
    "    if ncHGT: \n",
    "    #we consider the background size to be equal to the total # of genes \n",
    "    #(the sum of the weights of all entities would double count genes that occur in multiple sets\n",
    "    #... is this the right thing to do though?\n",
    "        background_gene_list_size = len(utils.csv2dict('../data/ID2gocam_mouse_ff.csv'))\n",
    "        \n",
    "    not_in_a_set, sets, setID2members_input_uni = get_sets(uni_list)\n",
    "    \n",
    "    setID2members_input = utils.map_dict_vals(uniprot2input, setID2members_input_uni)\n",
    "    \n",
    "    filtered_out1, set_list_filtered = filter_gene_list(sets,Dict)\n",
    "    filtered_out2, gene_list_filtered = filter_gene_list(uni_list, Dict) #need to clean gene_list to only include genes in the gocam\n",
    "    \n",
    "    \n",
    "    filtered_list = gene_list_filtered + set_list_filtered\n",
    "    gene_list_size = len(filtered_list)\n",
    "    \n",
    "    flist2input = {**uniprot2input, **setID2members_input}\n",
    "    filtered_list_as_genes = set(pd.Series(list(filtered_list)).map(flist2input).explode())\n",
    "    filtered_out_genes = set(gene_list) - filtered_list_as_genes\n",
    "    \n",
    "    counts = count_genes(filtered_list, Dict)\n",
    "    \n",
    "    N_ncHGT = False\n",
    "    if ncHGT == True:\n",
    "        N_ncHGT = len(gene_list)-len(filtered_out_genes)\n",
    "        if N_ncHGT <= 0:\n",
    "            return \"error no genes found in gocams\"\n",
    "        \n",
    "    num_calls = hgt(counts, gocam_sizes, FDR, gene_list_size, background_gene_list_size, ncHGT=N_ncHGT, compress= compress)\n",
    "    \n",
    "    return num_calls\n",
    "    \n",
    "def enrich_wrapper(filename, id_type, return_all = False, method = 'set', show_significant=True,FDR=.05,fpath= '../test_data', \n",
    "                   display_gene_symbol = True, compress = True):\n",
    "    \"\"\" returns (gene_list, filtered_out_genes, filtered_list, setID2members_input_uni, setID2members_input, df_display)\"\"\"\n",
    "    #set method files\n",
    "    gcs = '../data/gocam_sizes_mouse.csv'\n",
    "    id2g = '../data/ID2gocam_mouse.csv'\n",
    "    \n",
    "    #standard method files\n",
    "    if method == 'standard':\n",
    "        gcs = '../data/gocam_sizes_mouse_ff.csv'\n",
    "        id2g = '../data/ID2gocam_mouse_ff.csv'\n",
    "    \n",
    "    gene_list = pd.read_csv(os.path.join(fpath,filename),header=None,names = ['g'])\n",
    "    \n",
    "    #normally not needed, but I found a bug where HSPA1A and HSPA1B are listed as synonyms, both in Simplemine and official sources like the Alliance\n",
    "    gene_list.drop_duplicates(inplace = True) \n",
    "    \n",
    "    gene_list_converted = []\n",
    "    uniprot2input = {}\n",
    "    not_converted = []\n",
    "    \n",
    "    #conversion to uniprot IDs not needed for a list of uniprot IDs\n",
    "    if id_type == 'uniprot':\n",
    "        gene_list_converted = gene_list.g\n",
    "        uniprot2input = pd.Series(gene_list_converted.values,index=gene_list_converted).to_dict()\n",
    "    else:\n",
    "        gene_list_converted, uniprot2input, not_converted = utils.convert_IDs(gene_list,id_type)\n",
    "    \n",
    "    #read in dictionary and the gocam sizes\n",
    "    x = pd.read_csv(gcs)\n",
    "    gocam_sizes = pd.Series(x.sizes.values,index=x.gocam)\n",
    "    Dict = utils.csv2dict(id2g)\n",
    "    \n",
    "    #call enrich()\n",
    "    ncHGT = False\n",
    "    if method == 'ncHGT':\n",
    "        ncHGT = True\n",
    "    #results: (filtered_out_genes, filtered_list, setID2members_input_uni, setID2members_input, df_display)\n",
    "    num_calls = enrich(list(gene_list.g), gene_list_converted, uniprot2input, gocam_sizes, Dict, ncHGT = ncHGT, \n",
    "                       show_significant = show_significant, FDR=FDR, compress = compress)\n",
    "    return num_calls\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8f6b6-54b9-40aa-91a3-3134da40eb03",
   "metadata": {},
   "source": [
    "# Examining the number of calls for the LMNA_comb dataset between the compressed and uncompressed methods.\n",
    "\n",
    "looking both at specific pathways and also at the total number of calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bcaec-223f-4e71-b911-a3f98c8408b3",
   "metadata": {},
   "source": [
    "## Looking at the reduction on LNDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e6e276-8f88-4594-a4fb-e5ea8e408bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 326/326 [00:33<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "num_calls_F = enrich_wrapper('LMNA_comb.csv','Gene Symbol',method='ncHGT',FDR = 0.1,fpath = '../test_data/processed/', compress = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64387b1c-8ce4-478f-8e33-ef427d584301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 326/326 [00:01<00:00, 188.51it/s]\n"
     ]
    }
   ],
   "source": [
    "num_calls_T = enrich_wrapper('LMNA_comb.csv','Gene Symbol',method='ncHGT',FDR = 0.1,fpath = '../test_data/processed/', compress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88002e3a-3acd-41a0-b54c-0d165c27eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = np.array(num_calls_T)\n",
    "not_compressed = np.array(num_calls_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0b38dba-ed4c-4962-aca6-a3ca906c1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   16    47    50     5 11519    13     5    76    13   191]\n"
     ]
    }
   ],
   "source": [
    "print(compressed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8266337-2877-44cd-9867-2a2cde50b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     16      47      67       5 6291455      13       5     346      13\n",
      "     383]\n"
     ]
    }
   ],
   "source": [
    "print(not_compressed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48d6a257-152b-465b-8bf5-e86d388999f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.        ,   2.03537631,  10.7037283 , 546.18065804])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(np.divide(not_compressed,compressed), [0.5,.8,.9,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2dfd619-cf48-49f7-bae2-7e575634a1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01427793196010582"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(compressed)/np.sum(not_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993d576-aefa-4cbf-bc76-6bdc5d507199",
   "metadata": {},
   "source": [
    "# Median reduction in the number of calls to BiasedUrn per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36d36e1e-266a-4f83-aa8b-0d4a1ce5c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [00:25<00:00, 19.20it/s]\n",
      "100%|████████████████████████████████████████| 482/482 [00:02<00:00, 181.01it/s]\n",
      "100%|█████████████████████████████████████████| 436/436 [00:33<00:00, 12.85it/s]\n",
      "100%|████████████████████████████████████████| 436/436 [00:01<00:00, 257.66it/s]\n",
      "100%|█████████████████████████████████████████| 326/326 [00:33<00:00,  9.61it/s]\n",
      "100%|████████████████████████████████████████| 326/326 [00:01<00:00, 185.08it/s]\n",
      "100%|█████████████████████████████████████████| 185/185 [00:15<00:00, 11.74it/s]\n",
      "100%|████████████████████████████████████████| 185/185 [00:00<00:00, 223.33it/s]\n",
      "100%|█████████████████████████████████████████| 195/195 [00:31<00:00,  6.22it/s]\n",
      "100%|████████████████████████████████████████| 195/195 [00:00<00:00, 252.67it/s]\n",
      "100%|█████████████████████████████████████████| 268/268 [00:32<00:00,  8.28it/s]\n",
      "100%|████████████████████████████████████████| 268/268 [00:00<00:00, 276.01it/s]\n",
      "100%|████████████████████████████████████████| 324/324 [00:02<00:00, 112.58it/s]\n",
      "100%|████████████████████████████████████████| 324/324 [00:01<00:00, 209.69it/s]\n",
      "100%|█████████████████████████████████████████| 401/401 [00:33<00:00, 11.82it/s]\n",
      "100%|████████████████████████████████████████| 401/401 [00:02<00:00, 188.26it/s]\n",
      "100%|█████████████████████████████████████████| 358/358 [00:17<00:00, 20.55it/s]\n",
      "100%|████████████████████████████████████████| 358/358 [00:01<00:00, 213.67it/s]\n",
      "100%|█████████████████████████████████████████| 283/283 [00:32<00:00,  8.65it/s]\n",
      "100%|████████████████████████████████████████| 283/283 [00:01<00:00, 264.26it/s]\n",
      "100%|█████████████████████████████████████████| 461/461 [00:34<00:00, 13.39it/s]\n",
      "100%|████████████████████████████████████████| 461/461 [00:02<00:00, 185.39it/s]\n",
      "100%|█████████████████████████████████████████| 452/452 [00:35<00:00, 12.82it/s]\n",
      "100%|████████████████████████████████████████| 452/452 [00:02<00:00, 174.84it/s]\n",
      "100%|████████████████████████████████████████| 109/109 [00:00<00:00, 427.88it/s]\n",
      "100%|████████████████████████████████████████| 109/109 [00:00<00:00, 701.40it/s]\n",
      "100%|█████████████████████████████████████████| 254/254 [00:18<00:00, 14.02it/s]\n",
      "100%|████████████████████████████████████████| 254/254 [00:01<00:00, 224.45it/s]\n",
      "100%|█████████████████████████████████████████| 163/163 [00:17<00:00,  9.27it/s]\n",
      "100%|████████████████████████████████████████| 163/163 [00:00<00:00, 362.17it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:01<00:00, 52.04it/s]\n",
      "100%|██████████████████████████████████████████| 89/89 [00:00<00:00, 636.55it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [00:19<00:00, 24.69it/s]\n",
      "100%|████████████████████████████████████████| 480/480 [00:02<00:00, 239.02it/s]\n",
      "100%|█████████████████████████████████████████| 667/667 [00:35<00:00, 18.80it/s]\n",
      "100%|████████████████████████████████████████| 667/667 [00:03<00:00, 176.13it/s]\n",
      "100%|██████████████████████████████████████████| 98/98 [00:00<00:00, 416.06it/s]\n",
      "100%|██████████████████████████████████████████| 98/98 [00:00<00:00, 691.28it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '../test_data/processed'\n",
    "\n",
    "datasets = {'Covid-19 Platelets':('Gene Symbol',['platelets_up.csv','platelets_down.csv']),\n",
    "            'DCM Cardiomyocytes':('Gene Symbol',['LMNA_comb.csv','PKP2_comb.csv','RBM20_comb.csv','TTN_comb.csv','PVneg_comb.csv']),\n",
    "            'DCM Fibroblasts':('Gene Symbol',['LMNA_FB_comb.csv','PKP2_FB_comb.csv','RBM20_FB_comb.csv','TTN_FB_comb.csv','PVneg_FB_comb.csv']),\n",
    "            'Aging Brain Astrocytes':('Gene Symbol',['astro_HTH_up.csv','astro_CB_up.csv','astro_HTH_down.csv','astro_CB_down.csv']),\n",
    "            'P97 Inhibitor':('uniprot',['P97.csv']),\n",
    "            'Macrophage':('Gene Symbol',['mac_comb.csv']),\n",
    "            'NASH':('Gene Symbol',['Goavere_S2.csv'])\n",
    "           }\n",
    "\n",
    "total_calls = []\n",
    "for paper,val in datasets.items():\n",
    "    symbol_type = val[0]\n",
    "    datasets_ = val[1]\n",
    "    \n",
    "    for dataset in datasets_:\n",
    "        filename = os.path.join(path,dataset)\n",
    "        \n",
    "        num_calls_F = enrich_wrapper(filename,symbol_type,method='ncHGT', compress = False)\n",
    "        num_calls_T = enrich_wrapper(filename,symbol_type,method='ncHGT', compress = True)\n",
    "\n",
    "        compressed = np.array(num_calls_T)\n",
    "        not_compressed = np.array(num_calls_F)\n",
    "        total_calls.append(np.sum(compressed)/np.sum(not_compressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57484f8c-03f8-4124-bb46-24a6553dd9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015445306496629257"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(total_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "859c702f-28e9-435f-bfec-39f215fed7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.74458763367612"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.median(total_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb42394-e345-4d71-8096-ed52a596c972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gocam2]",
   "language": "python",
   "name": "conda-env-gocam2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
